{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9589700,"sourceType":"datasetVersion","datasetId":5848669}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":16.941997,"end_time":"2024-10-11T15:41:04.467967","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-11T15:40:47.52597","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import namedtuple","metadata":{"papermill":{"duration":0.026306,"end_time":"2024-10-11T15:40:50.501355","exception":false,"start_time":"2024-10-11T15:40:50.475049","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:31.768326Z","iopub.execute_input":"2024-10-16T06:42:31.768845Z","iopub.status.idle":"2024-10-16T06:42:31.774944Z","shell.execute_reply.started":"2024-10-16T06:42:31.768788Z","shell.execute_reply":"2024-10-16T06:42:31.773516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nimport cv2\n\nimport os\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport json\n\nfrom PIL import Image\n\nfrom matplotlib.patches import Polygon","metadata":{"papermill":{"duration":4.331777,"end_time":"2024-10-11T15:40:54.841927","exception":false,"start_time":"2024-10-11T15:40:50.51015","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:34.876303Z","iopub.execute_input":"2024-10-16T06:42:34.876789Z","iopub.status.idle":"2024-10-16T06:42:35.106686Z","shell.execute_reply.started":"2024-10-16T06:42:34.876745Z","shell.execute_reply":"2024-10-16T06:42:35.105417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import Compose\nimport torch.nn as nn","metadata":{"papermill":{"duration":2.033144,"end_time":"2024-10-11T15:40:56.883123","exception":false,"start_time":"2024-10-11T15:40:54.849979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:39.025497Z","iopub.execute_input":"2024-10-16T06:42:39.026100Z","iopub.status.idle":"2024-10-16T06:42:40.644569Z","shell.execute_reply.started":"2024-10-16T06:42:39.026050Z","shell.execute_reply":"2024-10-16T06:42:40.643481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create File Path Dataframes\n\n\n\n- Contains the paths to images instead of images themselves","metadata":{"papermill":{"duration":0.007541,"end_time":"2024-10-11T15:40:56.944756","exception":false,"start_time":"2024-10-11T15:40:56.937215","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#--------------------------------------------------------------------------------\n\n# Definitions\n\n#--------------------------------------------------------------------------------\n\n\n\n# a label and all meta information\n\nLabel = namedtuple( 'Label' , [\n\n\n\n    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n\n                    # We use them to uniquely name a class\n\n\n\n    'id'          , # An integer ID that is associated with this label.\n\n                    # The IDs are used to represent the label in ground truth images\n\n                    # An ID of -1 means that this label does not have an ID and thus\n\n                    # is ignored when creating ground truth images (e.g. license plate).\n\n\n\n    'trainId'     , # An integer ID that overwrites the ID above, when creating ground truth\n\n                    # images for training.\n\n                    # For training, multiple labels might have the same ID. Then, these labels\n\n                    # are mapped to the same class in the ground truth images. For the inverse\n\n                    # mapping, we use the label that is defined first in the list below.\n\n                    # For example, mapping all void-type classes to the same ID in training,\n\n                    # might make sense for some approaches.\n\n\n\n    'category'    , # The name of the category that this label belongs to\n\n\n\n    'categoryId'  , # The ID of this category. Used to create ground truth images\n\n                    # on category level.\n\n\n\n    'hasInstances', # Whether this label distinguishes between single instances or not\n\n\n\n    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n\n                    # during evaluations or not\n\n\n\n    'color'       , # The color of this label\n\n    ] )\n\n\n\n\n\n#--------------------------------------------------------------------------------\n\n# A list of all labels\n\n#--------------------------------------------------------------------------------\n\n\n\n# Please adapt the train IDs as appropriate for you approach.\n\n# Note that you might want to ignore labels with ID 255 during training.\n\n# Make sure to provide your results using the original IDs and not the training IDs.\n\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\n\n\nlabels = [\n\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n\n    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n\n    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n\n    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n\n    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n\n    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n\n    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n\n    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n\n    Label(  'road'                 ,  7 ,        0 , 'ground'          , 1       , False        , False        , (128, 64,128) ),\n\n    Label(  'sidewalk'             ,  8 ,        1 , 'ground'          , 1       , False        , False        , (244, 35,232) ),\n\n    Label(  'parking'              ,  9 ,      255 , 'ground'          , 1       , False        , True         , (250,170,160) ),\n\n    Label(  'rail track'           , 10 ,      255 , 'ground'          , 1       , False        , True         , (230,150,140) ),\n\n    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n\n    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n\n    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n\n    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n\n    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n\n    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n\n    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n\n    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n\n    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n\n    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n\n    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n\n    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n\n    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n\n    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n\n    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n\n    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n\n    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n\n    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n\n    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n\n    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n\n    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n\n    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n\n    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n\n    Label(  'license plate'        , 34 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n\n]","metadata":{"papermill":{"duration":0.037132,"end_time":"2024-10-11T15:40:56.928288","exception":false,"start_time":"2024-10-11T15:40:56.891156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:43.484900Z","iopub.execute_input":"2024-10-16T06:42:43.485556Z","iopub.status.idle":"2024-10-16T06:42:43.516819Z","shell.execute_reply.started":"2024-10-16T06:42:43.485509Z","shell.execute_reply":"2024-10-16T06:42:43.515716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate file paths for train dataset\n\ndef get_train_file_paths(images_dir, labels_dir):\n\n    data = []\n\n    # Iterate through cities in the train images folder\n\n    for city in os.listdir(images_dir):\n\n        city_image_dir = os.path.join(images_dir, city)\n\n        city_label_dir = os.path.join(labels_dir, city)\n\n        if os.path.isdir(city_image_dir) and os.path.isdir(city_label_dir):\n\n            for file in os.listdir(city_image_dir):\n\n                if file.endswith('_leftImg8bit.png'):\n\n                    image_path = os.path.join(city_image_dir, file)\n\n                    image_name = file.replace('_leftImg8bit.png', '')\n\n                    image_label_path = os.path.join(city_label_dir, f'{image_name}_gtFine_labelTrainIds.png')\n\n                    image_polygons_path = os.path.join(city_label_dir, f'{image_name}_gtFine_polygons.json')\n\n                    data.append([image_path, image_label_path, image_polygons_path])\n\n    return pd.DataFrame(data, columns=['image_path', 'image_label_path', 'image_polygons_path'])\n\n\n\n# Function to generate file paths for test dataset\n\ndef get_test_file_paths(test_dir):\n\n    data = []\n\n    # Iterate through cities in the test images folder\n\n    for city in os.listdir(test_dir):\n\n        city_dir = os.path.join(test_dir, city)\n\n        if os.path.isdir(city_dir):\n\n            for file in os.listdir(city_dir):\n\n                if file.endswith('_leftImg8bit.png'):\n\n                    image_path = os.path.join(city_dir, file)\n\n                    data.append([image_path])\n\n    return pd.DataFrame(data, columns=['image_path'])\n\n\n\n# Paths to images and labels directories\n\nimages_dir_train = '/kaggle/input/cityscapes-segmentation/images/train'\n\nlabels_dir_train = '/kaggle/input/cityscapes-segmentation/labels/train'\n\nimages_dir_test = '/kaggle/input/cityscapes-segmentation/images/test'\n\n\n\n# Create train and test DataFrames\n\ntrain_df = get_train_file_paths(images_dir_train, labels_dir_train)\n\ntest_df = get_test_file_paths(images_dir_test)","metadata":{"papermill":{"duration":1.536603,"end_time":"2024-10-11T15:40:58.489475","exception":false,"start_time":"2024-10-11T15:40:56.952872","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:48.857220Z","iopub.execute_input":"2024-10-16T06:42:48.857682Z","iopub.status.idle":"2024-10-16T06:42:50.455077Z","shell.execute_reply.started":"2024-10-16T06:42:48.857639Z","shell.execute_reply":"2024-10-16T06:42:50.453810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example for the paths","metadata":{"papermill":{"duration":0.007561,"end_time":"2024-10-11T15:40:58.505188","exception":false,"start_time":"2024-10-11T15:40:58.497627","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(train_df['image_path'][100])\n\nprint(train_df['image_label_path'][100])\n\nprint(train_df['image_polygons_path'][100])","metadata":{"papermill":{"duration":0.02257,"end_time":"2024-10-11T15:40:58.53569","exception":false,"start_time":"2024-10-11T15:40:58.51312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:53.643362Z","iopub.execute_input":"2024-10-16T06:42:53.643939Z","iopub.status.idle":"2024-10-16T06:42:53.652154Z","shell.execute_reply.started":"2024-10-16T06:42:53.643887Z","shell.execute_reply":"2024-10-16T06:42:53.650788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{"papermill":{"duration":0.007691,"end_time":"2024-10-11T15:40:58.551736","exception":false,"start_time":"2024-10-11T15:40:58.544045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def visualize_train_row(row):\n\n    # Load the image\n\n    image = Image.open(row['image_path'])\n\n    \n\n    # Load the label image (segmentation map)\n\n    label_image = Image.open(row['image_label_path'])\n\n\n\n    # Visualize the image and label side by side\n\n    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n\n\n\n    # Display the original image\n\n    ax[0].imshow(image)\n\n    ax[0].set_title('Original Image')\n\n    ax[0].axis('off')\n\n\n\n    # Display the label (segmentation) image\n\n    ax[1].imshow(label_image)\n\n    ax[1].set_title('Segmentation Label Image')\n\n    ax[1].axis('off')\n\n\n\n    plt.show()\n\n\n\n\n\n# Example usage with a row from train_df\n\nvisualize_train_row(train_df.iloc[1000])\n","metadata":{"papermill":{"duration":1.153667,"end_time":"2024-10-11T15:40:59.713257","exception":false,"start_time":"2024-10-11T15:40:58.55959","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:42:56.423645Z","iopub.execute_input":"2024-10-16T06:42:56.424231Z","iopub.status.idle":"2024-10-16T06:42:57.406354Z","shell.execute_reply.started":"2024-10-16T06:42:56.424171Z","shell.execute_reply":"2024-10-16T06:42:57.404888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_train_row_with_polygons(row):\n\n    # Load the original image\n\n    image = Image.open(row['image_path'])\n\n\n\n    # Load and parse the polygons JSON\n\n    with open(row['image_polygons_path'], 'r') as f:\n\n        polygons_data = json.load(f)\n\n    \n\n    # Visualize the image\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    \n\n    # Display the original image\n\n    ax.imshow(image)\n\n    # Iterate through the objects in the JSON file\n\n    for obj in polygons_data['objects']:\n\n        polygon_coords = obj['polygon']\n\n        \n\n        # Create a Polygon patch with the coordinates from JSON\n\n        polygon = Polygon(polygon_coords, closed=True, edgecolor='red', fill=False, linewidth=1)\n\n        \n\n        # Add the polygon patch to the image plot\n\n        ax.add_patch(polygon)\n\n    \n\n    # Set title and show the image with polygons overlaid\n\n    ax.set_title('Original Image with Polygons')\n\n    ax.axis('off')\n\n    \n\n    plt.show()\n\n\n\n    #Print the JSON polygons data for reference\n\n    # print(\"Polygons data from JSON file:\")\n\n    # print(json.dumps(polygons_data, indent=4))\n\n\n\n# Example usage with a row from train_df\n\nvisualize_train_row_with_polygons(train_df.iloc[1000])\n","metadata":{"papermill":{"duration":1.12305,"end_time":"2024-10-11T15:41:00.849249","exception":false,"start_time":"2024-10-11T15:40:59.726199","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:43:00.141016Z","iopub.execute_input":"2024-10-16T06:43:00.141583Z","iopub.status.idle":"2024-10-16T06:43:01.212471Z","shell.execute_reply.started":"2024-10-16T06:43:00.141530Z","shell.execute_reply":"2024-10-16T06:43:01.211232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Val/Train Split + Saving to CSV","metadata":{"papermill":{"duration":0.020079,"end_time":"2024-10-11T15:41:00.890353","exception":false,"start_time":"2024-10-11T15:41:00.870274","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#TODO : Split the dataset into training and validation data\n\nprint(\"Number of Samples before Split: \", len(train_df))\n\n\n\n# Shuffle the DataFrame\n\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\n\n\n\n# Calculate the number of samples for\n\nval_size = 380\n\n\n\n# Split the DataFrame into train and validation sets\n\ntrain_df_final = train_df[:-val_size]\n\nval_df_final = train_df[-val_size:]\n\n\n\nprint(\"Train Samples (After Split): \", len(train_df_final))\n\nprint(\"Val Samples (After Split): \", len(val_df_final))\n\n\n\n#TODO : Save the split to csv file\n\ntrain_df_final.to_csv(\"train_data.csv\", index=False)\n\nval_df_final.to_csv(\"val_data.csv\", index=False)\n\ntest_df.to_csv(\"test_data.csv\", index=False)","metadata":{"papermill":{"duration":0.064112,"end_time":"2024-10-11T15:41:00.974861","exception":false,"start_time":"2024-10-11T15:41:00.910749","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T06:43:05.356093Z","iopub.execute_input":"2024-10-16T06:43:05.356557Z","iopub.status.idle":"2024-10-16T06:43:05.420653Z","shell.execute_reply.started":"2024-10-16T06:43:05.356514Z","shell.execute_reply":"2024-10-16T06:43:05.419298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PRINT SAMPLE FROM CSV_FILE","metadata":{"papermill":{"duration":0.029766,"end_time":"2024-10-11T15:41:01.024934","exception":false,"start_time":"2024-10-11T15:41:00.995168","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:16:28.338554Z","iopub.execute_input":"2024-10-15T12:16:28.339016Z","iopub.status.idle":"2024-10-15T12:16:28.343239Z","shell.execute_reply.started":"2024-10-15T12:16:28.338977Z","shell.execute_reply":"2024-10-15T12:16:28.342056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis\n\n\n\n### Write Most Important Notes Here:\n\n\n\n- Image dimensions are (1024, 2048, 3)\n\n- Mask is of shape (1024, 2048)\n\n- Datatypes for both are uint8","metadata":{"papermill":{"duration":0.019904,"end_time":"2024-10-11T15:41:01.065622","exception":false,"start_time":"2024-10-11T15:41:01.045718","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#TODO: write your own data analysis techniques","metadata":{"papermill":{"duration":0.029462,"end_time":"2024-10-11T15:41:01.115246","exception":false,"start_time":"2024-10-11T15:41:01.085784","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:16:28.344542Z","iopub.execute_input":"2024-10-15T12:16:28.344952Z","iopub.status.idle":"2024-10-15T12:16:28.353294Z","shell.execute_reply.started":"2024-10-15T12:16:28.344889Z","shell.execute_reply":"2024-10-15T12:16:28.352216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Pixel Class Frequency","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\n\ndef map_labels_to_trainIds(labels):\n    trainId_to_label = {label.trainId: label.name for label in labels if label.trainId != 255}\n    return trainId_to_label\n\ndef pixel_class_frequency(train_df: pd.DataFrame, mask_labels):\n    trainId_to_label = map_labels_to_trainIds(mask_labels)\n    \n    pixel_counts = Counter()\n    class_frequency = Counter()\n    \n    for index, row in train_df.iterrows():\n        mask_path = row[\"image_label_path\"]\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        \n        unique, counts = np.unique(mask, return_counts=True)\n        \n        for u, c in zip(unique, counts):\n            if u in trainId_to_label:\n                pixel_counts[trainId_to_label[u]] += c\n                class_frequency[trainId_to_label[u]] += 1\n                \n    return pixel_counts, class_frequency","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:43:10.373625Z","iopub.execute_input":"2024-10-16T06:43:10.374089Z","iopub.status.idle":"2024-10-16T06:43:10.384954Z","shell.execute_reply.started":"2024-10-16T06:43:10.374043Z","shell.execute_reply":"2024-10-16T06:43:10.383504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixel_class_distribution, class_distribution = pixel_class_frequency(train_df, labels)\nprint(pixel_class_distribution)\nprint(class_distribution)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:43:13.465222Z","iopub.execute_input":"2024-10-16T06:43:13.466238Z","iopub.status.idle":"2024-10-16T06:45:41.827780Z","shell.execute_reply.started":"2024-10-16T06:43:13.466179Z","shell.execute_reply":"2024-10-16T06:45:41.826344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport torch\nimport torch.nn as nn\n\npixel_class_distribution = Counter({\n    'road': 1626667099,\n    'building': 1010874745,\n    'vegetation': 701460788,\n    'car': 307246339,\n    'sidewalk': 270602244,\n    'sky': 175475731,\n    'pole': 53754015,\n    'person': 52224101,\n    'terrain': 51298793,\n    'fence': 38197514,\n    'wall': 30679368,\n    'traffic sign': 24204440,\n    'bicycle': 18262872,\n    'truck': 11426619,\n    'train': 11260221,\n    'bus': 11056793,\n    'traffic light': 9125136,\n    'rider': 5898636,\n    'motorcycle': 4282304\n})\n\ntotal_pixels = sum(pixel_class_distribution.values())\nnum_classes = len(pixel_class_distribution)\n\nclass_weights = {cls: total_pixels / (pixel_class_distribution * count) for cls, count in pixel_class_distribution.items()}\nclass_weights_tensor = torch.FloatTensor([class_weights[cls] for cls in pixel_class_distribution])\n\n# Define your loss function with class weights\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T07:58:41.206692Z","iopub.execute_input":"2024-10-16T07:58:41.208168Z","iopub.status.idle":"2024-10-16T07:58:41.219032Z","shell.execute_reply.started":"2024-10-16T07:58:41.208110Z","shell.execute_reply":"2024-10-16T07:58:41.217773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Assuming class weights are calculated and converted to a tensor\nclass_weights_tensor = torch.FloatTensor([\n    class_weights['road'],\n    class_weights['building'],\n    class_weights['vegetation'],\n    class_weights['car'],\n    class_weights['sidewalk'],\n    class_weights['sky'],\n    class_weights['pole'],\n    class_weights['person'],\n    class_weights['terrain'],\n    class_weights['fence'],\n    class_weights['wall'],\n    class_weights['traffic sign'],\n    class_weights['bicycle'],\n    class_weights['truck'],\n    class_weights['train'],\n    class_weights['bus'],\n    class_weights['traffic light'],\n    class_weights['rider'],\n    class_weights['motorcycle'],\n]) \n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef plot_class_distribution(distribution, title, y_label):\n    df = pd.DataFrame(list(distribution.items()), columns=['Class', 'Count'])\n\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x='Class', y='Count', data=df, palette='coolwarm')\n\n    plt.xticks(rotation=90, ha='right')\n    plt.xlabel('Class')\n    plt.ylabel(y_label)\n    plt.title(title)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_class_distribution(pixel_class_distribution, title=\"Pixel-Wise Class Distribution\", y_label=\"Pixel Count\")\nplot_class_distribution(class_distribution, title=\"Images Class Distribution\", y_label=\"Images Count\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:53:26.287591Z","iopub.execute_input":"2024-10-16T06:53:26.288060Z","iopub.status.idle":"2024-10-16T06:53:27.191007Z","shell.execute_reply.started":"2024-10-16T06:53:26.288018Z","shell.execute_reply":"2024-10-16T06:53:27.189779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting Insights Over Images' Brightness","metadata":{}},{"cell_type":"code","source":"def calculate_luminance(train_df: pd.DataFrame):\n    images_lum = []\n    for index, row in train_df.iterrows():\n        image_path = row[\"image_path\"]\n        image = cv2.imread(image_path)\n        \n        image = image.astype('float32') / 255.0\n    \n        R, G, B = image[:,:,2], image[:,:,1], image[:,:,0]\n    \n        luminance = 0.299 * R + 0.587 * G + 0.114 * B\n        avg_luminance = np.mean(luminance)\n        \n        images_lum.append(avg_luminance)\n    return images_lum","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:46:11.298564Z","iopub.execute_input":"2024-10-16T06:46:11.299037Z","iopub.status.idle":"2024-10-16T06:46:11.308263Z","shell.execute_reply.started":"2024-10-16T06:46:11.298991Z","shell.execute_reply":"2024-10-16T06:46:11.306854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_luminance_frequency(lum_vals):\n    lum_vals = np.array(lum_vals)\n    lum_vals = lum_vals[np.isfinite(lum_vals)]\n    \n    luminance_df = pd.DataFrame({'Luminance': lum_vals})\n    \n    plt.figure(figsize=(12, 6))\n    sns.histplot(luminance_df['Luminance'], bins=15, kde=True)\n    plt.title('Histogram of Average Luminance Values')\n    plt.xlabel('Average Luminance')\n    plt.ylabel('Frequency')\n    plt.xlim(0, 1)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:46:14.131217Z","iopub.execute_input":"2024-10-16T06:46:14.132336Z","iopub.status.idle":"2024-10-16T06:46:14.140006Z","shell.execute_reply.started":"2024-10-16T06:46:14.132279Z","shell.execute_reply":"2024-10-16T06:46:14.138801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_lum = calculate_luminance(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:46:17.267708Z","iopub.execute_input":"2024-10-16T06:46:17.268124Z","iopub.status.idle":"2024-10-16T06:51:06.749900Z","shell.execute_reply.started":"2024-10-16T06:46:17.268085Z","shell.execute_reply":"2024-10-16T06:51:06.748379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_luminance_frequency(images_lum)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:52:47.061018Z","iopub.execute_input":"2024-10-16T06:52:47.061573Z","iopub.status.idle":"2024-10-16T06:52:47.517066Z","shell.execute_reply.started":"2024-10-16T06:52:47.061525Z","shell.execute_reply":"2024-10-16T06:52:47.515861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.020163,"end_time":"2024-10-11T15:41:01.15636","exception":false,"start_time":"2024-10-11T15:41:01.136197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Example for the desired interface \n\n\n\nclass LoadImage():\n\n   def __init__(self, keys):\n\n       self.keys=keys\n\n   def __call__(self,sample):\n\n       for key in self.keys:\n\n           sample[key]= sample[key]\n\n       return sample","metadata":{"papermill":{"duration":0.030802,"end_time":"2024-10-11T15:41:01.207565","exception":false,"start_time":"2024-10-11T15:41:01.176763","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T07:44:57.162228Z","iopub.execute_input":"2024-10-16T07:44:57.163252Z","iopub.status.idle":"2024-10-16T07:44:57.174212Z","shell.execute_reply.started":"2024-10-16T07:44:57.163154Z","shell.execute_reply":"2024-10-16T07:44:57.172613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO : TRAIN DATA PREPROCESSING PIPELINE\n\nfrom torchvision import transforms\n\nTrain_data_transform = transforms.Compose([\n    LoadImage(['image','mask']),\n\n    # Write your own data transforms and augmentations\n    \n    # Random rotation to help with orientation variance\n    #transforms.RandomRotation(degrees=15),\n    \n    # Randomly resize and crop the image to introduce scale variance\n    # transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n    \n    \n    \n    transforms.RandomChoice([\n        transforms.ColorJitter(brightness=(1.2, 1.8), contrast=(0.5, 1.5), saturation=(1.2, 1.5), hue=0.2),\n        transforms.ColorJitter(brightness=(1.2, 1.5), contrast=(1.2, 2), saturation=(0.5, 0.8), hue=0.2),\n    ]), # Chooses one of the color space transforms\n    transforms.RandomAdjustSharpness(sharpness_factor=0.3),\n    transforms.RandomHorizontalFlip(p=0.8),\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.3),\n    transforms.ToTensor(),\n])\n\n#TODO : VALIDATION DATA PREPROCESSING PIPELINE\nValid_data_transform=Compose([\n\n    LoadImage(['image','mask']),\n\n    # Write your own data transforms and augmentations\n    #transforms.Resize((256, 256)),  # Ensure the size matches the model's input\n    #transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Match normalization with training\n\n\n\n])\n\n\n\n#TODO : TEST DATA PREPROCESSING PIPELINE\n\n\n\n\n\nTest_data_transform=Compose([\n\n    LoadImage(['image']), # No mask since it is a test image\n\n    # Write your own data transforms and augmentations\n    # transforms.Resize((256, 256)),  # Ensure the size matches the model's input\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Match normalization\n\n\n\n])","metadata":{"papermill":{"duration":0.031045,"end_time":"2024-10-11T15:41:01.259247","exception":false,"start_time":"2024-10-11T15:41:01.228202","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T07:45:00.957138Z","iopub.execute_input":"2024-10-16T07:45:00.957603Z","iopub.status.idle":"2024-10-16T07:45:00.970865Z","shell.execute_reply.started":"2024-10-16T07:45:00.957561Z","shell.execute_reply":"2024-10-16T07:45:00.969547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nfrom torch.utils.data import DataLoader, Dataset\n\nimport pandas as pd\n\nfrom typing import List\n\nfrom torchvision.transforms import Compose\n\n\n\nclass Dataset(Dataset):\n\n    def __init__(self, input_dataframe: pd.DataFrame, data_transform:Compose, has_labels=True):\n\n        self.input_dataframe = input_dataframe\n\n        self.data_transform = data_transform\n\n        self.has_labels = has_labels\n\n\n\n    def __getitem__(self, item: int):\n\n        sample_idx = item\n\n        sample = {}\n\n        sample['image'] = torch.from_numpy(cv2.imread(self.input_dataframe['image_path'][sample_idx], cv2.IMREAD_UNCHANGED))/255\n\n        if self.has_labels:\n\n           sample['mask'] = torch.from_numpy(cv2.imread(self.input_dataframe['image_label_path'][sample_idx], cv2.IMREAD_UNCHANGED))\n\n        sample = self.data_transform(sample)\n        \n        # Bring channel dimension first\n        sample['image'] = torch.permute(sample['image'], (2, 0, 1))\n        sample['mask'] = sample['mask'].to(torch.int64)\n\n        return sample\n\n        \n\n        \n\n    def __len__(self):\n\n        return len(self.input_dataframe)","metadata":{"papermill":{"duration":0.034302,"end_time":"2024-10-11T15:41:01.314113","exception":false,"start_time":"2024-10-11T15:41:01.279811","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T07:45:06.031671Z","iopub.execute_input":"2024-10-16T07:45:06.032141Z","iopub.status.idle":"2024-10-16T07:45:06.045190Z","shell.execute_reply.started":"2024-10-16T07:45:06.032097Z","shell.execute_reply":"2024-10-16T07:45:06.043636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO : Initilize your datasets\n\n\n\nds_train=Dataset(input_dataframe=train_df_final,\n\n                data_transform=Train_data_transform)\n","metadata":{"papermill":{"duration":0.029571,"end_time":"2024-10-11T15:41:01.364259","exception":false,"start_time":"2024-10-11T15:41:01.334688","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T07:45:15.428034Z","iopub.execute_input":"2024-10-16T07:45:15.428494Z","iopub.status.idle":"2024-10-16T07:45:15.435063Z","shell.execute_reply.started":"2024-10-16T07:45:15.428447Z","shell.execute_reply":"2024-10-16T07:45:15.433683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"dl_train=DataLoader(dataset=ds_train,batch_size= 2 ,num_workers=4 ,prefetch_factor=8,shuffle=True)","metadata":{"papermill":{"duration":0.031327,"end_time":"2024-10-11T15:41:01.415801","exception":false,"start_time":"2024-10-11T15:41:01.384474","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T07:45:39.190084Z","iopub.execute_input":"2024-10-16T07:45:39.190619Z","iopub.status.idle":"2024-10-16T07:45:39.198394Z","shell.execute_reply.started":"2024-10-16T07:45:39.190570Z","shell.execute_reply":"2024-10-16T07:45:39.196915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO : Show samples from your data loaders\n\ndef print_batch_info(data_loader):\n\n  \"\"\"Prints information about batches from a DataLoader.\n\n\n\n  Args:\n\n    data_loader: The DataLoader to inspect.\n\n  \"\"\"\n\n\n\n  for batch in data_loader:\n\n    print(\"Batch Images shape:\", batch['image'].shape)\n\n    print(\"Batch Masks shape:\", batch['mask'].shape)\n\n    print(\"Batch Images dtype:\", batch['image'].dtype)\n\n    print(\"Batch Masks dtype:\", batch['mask'].dtype)\n\n    print(\"Batch Image device:\", batch['image'].device)\n\n    print(\"Batch Masks device:\", batch['mask'].device)\n\n    print(\"\\n\")\n\n    break # print for first batch only\n\n\n\n# Example usage\n\ndl_train = DataLoader(dataset=ds_train, batch_size=2, num_workers=4, prefetch_factor=8, shuffle=True)\n\n\n\nprint_batch_info(dl_train)","metadata":{"papermill":{"duration":1.486841,"end_time":"2024-10-11T15:41:02.923371","exception":false,"start_time":"2024-10-11T15:41:01.43653","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-16T07:49:11.340959Z","iopub.execute_input":"2024-10-16T07:49:11.341478Z","iopub.status.idle":"2024-10-16T07:49:11.926075Z","shell.execute_reply.started":"2024-10-16T07:49:11.341434Z","shell.execute_reply":"2024-10-16T07:49:11.921318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model (U-NET)","metadata":{}},{"cell_type":"code","source":"#TODO : Write the model you are going to use (Pytorch)\n\n\nclass UNet(nn.Module):\n    def double_convolution(self, in_channels, out_channels):\n        conv_op = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        return conv_op\n    \n    def __init__(self, num_classes=20):\n        super(UNet, self).__init__()\n        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        # Contracting path.\n        \n        # Each convolution is applied twice.\n        self.down_convolution_1 = self.double_convolution(3, 16)\n        self.down_convolution_2 = self.double_convolution(16, 32)\n        self.down_convolution_3 = self.double_convolution(32, 64)\n        self.down_convolution_4 = self.double_convolution(64, 128)\n        self.down_convolution_5 = self.double_convolution(128, 256)\n        \n        \n        \n        # Expanding path.\n        self.up_transpose_1 = nn.ConvTranspose2d(\n            in_channels=256, out_channels=128,\n            kernel_size=2, \n            stride=2)\n        self.up_convolution_1 = self.double_convolution(256, 128)\n        \n        \n        self.up_transpose_2 = nn.ConvTranspose2d(\n            in_channels=128, out_channels=64,\n            kernel_size=2, \n            stride=2)\n        self.up_convolution_2 = self.double_convolution(128, 64)\n        \n        \n        self.up_transpose_3 = nn.ConvTranspose2d(\n            in_channels=64, out_channels=32,\n            kernel_size=2, \n            stride=2)\n        self.up_convolution_3 = self.double_convolution(64, 32)\n        \n        \n        self.up_transpose_4 = nn.ConvTranspose2d(\n            in_channels=32, out_channels=16,\n            kernel_size=2, \n            stride=2)\n        self.up_convolution_4 = self.double_convolution(32, 16)\n        \n\n        self.out = nn.Conv2d(\n            in_channels=16, out_channels=num_classes, \n            kernel_size=1\n        )\n        \n\n        \n    def forward(self, x):\n        # Input x shape is [batch_size, 3, 1024, 2048]\n        \n        # Contracting\n        down_1 = self.down_convolution_1(x) \n        down_2 = self.max_pool2d(down_1) \n        down_3 = self.down_convolution_2(down_2)\n        down_4 = self.max_pool2d(down_3)\n        down_5 = self.down_convolution_3(down_4) \n        down_6 = self.max_pool2d(down_5) \n        down_7 = self.down_convolution_4(down_6) \n        down_8 = self.max_pool2d(down_7)\n        down_9 = self.down_convolution_5(down_8) # [batch_size, 256, 64, 128]\n        \n        \n        # Expanding\n        up_1 = self.up_transpose_1(down_9) # [batch_size, 128, 128, 256]\n        x = self.up_convolution_1(torch.cat([down_7, up_1], 1))\n        up_2 = self.up_transpose_2(x)\n        x = self.up_convolution_2(torch.cat([down_5, up_2], 1))\n        up_3 = self.up_transpose_3(x)\n        x = self.up_convolution_3(torch.cat([down_3, up_3], 1))\n        up_4 = self.up_transpose_4(x) \n        x = self.up_convolution_4(torch.cat([down_1, up_4], 1)) # [batch_size, 16, 1024, 2048]\n        \n        out = self.out(x) # [batch_size, num_classes, 1024, 2048]\n        return out\n\n\n\n    \n    \n# # Test for the model\n# model = UNet()\n# output = None\n# inp = None\n# label = None\n# for batch in dl_train:\n#     inp = batch['image']\n#     label = batch['mask']\n#     output = model(batch['image'])\n#     break\n\n","metadata":{"papermill":{"duration":0.030724,"end_time":"2024-10-11T15:41:02.974753","exception":false,"start_time":"2024-10-11T15:41:02.944029","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:18:50.780633Z","iopub.execute_input":"2024-10-15T12:18:50.781023Z","iopub.status.idle":"2024-10-15T12:18:50.798994Z","shell.execute_reply.started":"2024-10-15T12:18:50.780983Z","shell.execute_reply":"2024-10-15T12:18:50.79771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"#TODO : Write the loss function you are going to use\nimport torch.optim as optim\n\nmodel = UNet()\nloss = nn.CrossEntropyLoss(ignore_index=255)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# # Test for the loss\n# print(output.shape)\n# print(inp.shape)\n# print(label.shape)\n# print(output.dtype)\n# print(inp.dtype)\n# print(label.dtype)\n\n\n# print(loss(output,label))\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"papermill":{"duration":0.028878,"end_time":"2024-10-11T15:41:03.024279","exception":false,"start_time":"2024-10-11T15:41:02.995401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:18:50.800598Z","iopub.execute_input":"2024-10-15T12:18:50.80101Z","iopub.status.idle":"2024-10-15T12:18:50.846203Z","shell.execute_reply.started":"2024-10-15T12:18:50.800973Z","shell.execute_reply":"2024-10-15T12:18:50.844938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO : Write the evaluation metrics you are going to use\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\ndef evaluate_unet_model(model, dataloader, device):\n    model = model.to(device)\n    model.eval()  # Set model to evaluation mode\n\n    # Initialize metrics\n    total_correct = 0\n    total_pixels = 0\n    class_intersection = np.zeros((num_classes,))\n    class_union = np.zeros((num_classes,))\n\n    with torch.no_grad():  # Disable gradient calculation\n        for batch in tqdm(dataloader):\n            inputs = batch['image'].to(device)\n            masks = batch['mask'].to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, dim=1)  # Get predictions\n\n            # Calculate pixel-wise accuracy\n            total_correct += (preds == masks).sum().item()\n            total_pixels += masks.numel()\n\n            # Calculate per-class IoU\n            for c in range(num_classes):\n                intersection = ((preds == c) & (masks == c)).sum().item()\n                union = ((preds == c) | (masks == c)).sum().item()\n\n                class_intersection[c] += intersection\n                class_union[c] += union\n\n    # Calculate overall accuracy\n    pixel_accuracy = total_correct / total_pixels\n\n    # Calculate mean IoU for each class\n    mean_iou = class_intersection / (class_union + 1e-6)  # Avoid division by zero\n    mean_iou = np.nanmean(mean_iou)  # Get the mean, ignoring NaNs\n\n    print(f'Pixel Accuracy: {pixel_accuracy:.4f}')\n    print(f'Mean IoU: {mean_iou:.4f}')\n\n# Example usage\n# Assuming you have a validation dataloader named `dl_val`\nevaluate_unet_model(model, dl_val, device)\n","metadata":{"papermill":{"duration":0.029866,"end_time":"2024-10-11T15:41:03.074444","exception":false,"start_time":"2024-10-11T15:41:03.044578","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:18:50.847727Z","iopub.execute_input":"2024-10-15T12:18:50.848301Z","iopub.status.idle":"2024-10-15T12:18:50.853527Z","shell.execute_reply.started":"2024-10-15T12:18:50.848242Z","shell.execute_reply":"2024-10-15T12:18:50.852434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_unet_model(model, dataloader, criterion, optimizer, num_epochs=10, device=device):\n    model = model.to(device)\n    model.train()  # Set model to training mode\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n\n        # Progress bar for the epoch\n        pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n\n        for i, batch in pbar:\n            inputs = batch['image'].to(device)\n            masks = batch['mask'].to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = criterion(outputs, masks)\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Update running loss\n            running_loss += loss.item()\n            pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / (i+1):.4f}\")\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(dataloader):.4f}\")\n\n    print(\"Training complete\")\n","metadata":{"papermill":{"duration":0.029898,"end_time":"2024-10-11T15:41:03.124959","exception":false,"start_time":"2024-10-11T15:41:03.095061","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:18:50.85477Z","iopub.execute_input":"2024-10-15T12:18:50.855154Z","iopub.status.idle":"2024-10-15T12:18:50.869025Z","shell.execute_reply.started":"2024-10-15T12:18:50.855106Z","shell.execute_reply":"2024-10-15T12:18:50.867813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_unet_model(model, dl_train, loss, optimizer)  # training on 10 epochs","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:18:50.870612Z","iopub.execute_input":"2024-10-15T12:18:50.871081Z","iopub.status.idle":"2024-10-15T12:37:34.600734Z","shell.execute_reply.started":"2024-10-15T12:18:50.87103Z","shell.execute_reply":"2024-10-15T12:37:34.59839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO : Plot losses and metrics graphs","metadata":{"papermill":{"duration":0.029103,"end_time":"2024-10-11T15:41:03.174564","exception":false,"start_time":"2024-10-11T15:41:03.145461","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:37:34.601851Z","iopub.status.idle":"2024-10-15T12:37:34.602273Z","shell.execute_reply.started":"2024-10-15T12:37:34.602071Z","shell.execute_reply":"2024-10-15T12:37:34.602092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO : Test your model and show some samples","metadata":{"papermill":{"duration":0.028729,"end_time":"2024-10-11T15:41:03.223699","exception":false,"start_time":"2024-10-11T15:41:03.19497","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-15T12:37:34.603813Z","iopub.status.idle":"2024-10-15T12:37:34.604219Z","shell.execute_reply.started":"2024-10-15T12:37:34.604029Z","shell.execute_reply":"2024-10-15T12:37:34.604051Z"},"trusted":true},"execution_count":null,"outputs":[]}]}